{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim\n",
    "## Key ideas\n",
    "### Document\n",
    "A string type object\n",
    "### Corpus \n",
    "A list of docs\n",
    "### Vectors\n",
    "mathematical expressions of corpus\n",
    "### models\n",
    "tranformation of a matrix into another one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]\n",
    "\n",
    "stop_words = set(\"for a of the and to in\".split())\n",
    "# docs = [[word for word in doc.lower().split() if word not in stop_words] for doc in documents] \n",
    "\n",
    "# BoW\n",
    "\n",
    "def create_vocab(corpus):\n",
    "    vocab = set()\n",
    "    for doc in corpus:\n",
    "        for word in doc:\n",
    "            vocab.add(word)\n",
    "    return sorted(list(vocab))\n",
    "\n",
    "def build_bow_matrix(corpus, vocab):\n",
    "    # Create word to index mapping\n",
    "    word_index = {word: i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    # Initialize lists to construct sparse matrix\n",
    "    row_ind = []\n",
    "    col_ind = []\n",
    "    data = []\n",
    "    \n",
    "    # Fill the lists\n",
    "    for doc_id, doc in enumerate(corpus):\n",
    "        for word in doc:\n",
    "            if word in word_index:\n",
    "                row_ind.append(doc_id)\n",
    "                col_ind.append(word_index[word])\n",
    "                data.append(1)\n",
    "    \n",
    "    # Create sparse matrix\n",
    "    bow = sparse.csr_matrix((data, (row_ind, col_ind)), \n",
    "                          shape=(len(corpus), len(vocab)), \n",
    "                          dtype=np.float32)  # Use float32 instead of float64\n",
    "    \n",
    "    return bow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parenthesis about `yield`\n",
    "compare to a regular function that returns all the results in the same time which occupies memory, `yield` returns a generator in which values are accessible by iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from typing import Generator\n",
    "def regular_fun() -> list:\n",
    "    return [i for i in range(1000000000)]\n",
    "\n",
    "# f = regular_fun() dangeous!!!!!\n",
    "\n",
    "def yield_fun() -> Generator:\n",
    "    for i in range(1000000000):\n",
    "        yield i\n",
    "\n",
    "f = yield_fun()\n",
    "for i in f:\n",
    "    print(i)\n",
    "    if i > 5:\n",
    "        break\n",
    "\n",
    "def read_large_text(path):\n",
    "    with open(path,'w',encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            yield line.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step into LDA model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello world'\n",
      "<class 'bytes'>\n",
      "hello world\n",
      "<class 'str'>\n",
      "\n",
      "b'\\xc3\\xa9\\xc3\\xb4\\xe8\\xa1\\x8c'\n",
      "<class 'bytes'>\n",
      "\n",
      "éô行\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import smart_open\n",
    "\n",
    "def extract_documents(url):\n",
    "    with smart_open.open(url, \"rb\") as file: # this returns data as bytes\n",
    "        with tarfile.open(fileobj=file) as tar:\n",
    "            for member in tar.getmembers(): # members = number of files in the current floder\n",
    "                if member.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', member.name):\n",
    "                    member_bytes = tar.extractfile(member).read() \n",
    "                    yield member_bytes.decode('utf-8', errors='replace') # unrecognized character will be replaced by (U+FFFD)\n",
    "\n",
    "\n",
    "docs = list(extract_documents(\"../nips12raw_str602.tgz\"))\n",
    "\n",
    "byte_str = b\"hello world\"\n",
    "print(byte_str,type(byte_str),sep='\\n')\n",
    "\n",
    "decoded_str = byte_str.decode('utf-8')\n",
    "print(decoded_str,type(decoded_str),sep='\\n')\n",
    "print()\n",
    "\n",
    "nonascii = \"éô行\"\n",
    "encoded_text = nonascii.encode(\"utf-8\")\n",
    "print(encoded_text,type(encoded_text),sep='\\n')\n",
    "print()\n",
    "decoded_text = encoded_text.decode(\"utf-8\")\n",
    "print(decoded_text,type(decoded_text),sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing\n",
    "- Tokenization based on a regex tokenizer from `nltk`\n",
    "- Lemmatization\n",
    "- compute bigrams\n",
    "- build BoW\n",
    "- remove stopwords (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/chen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import RegexpTokenizer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stoplist = set(stopwords.words(\"english\"))\n",
    "tokenizer = RegexpTokenizer(r'\\w+') \n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx]) # each doc contains a list of tokens, the docs is a corpus contains a list of list, each list is a doc\n",
    "\n",
    "# remove numbers but not words that contain numbers\n",
    "docs = [[token for token in doc if not token.isnumeric() and token not in stoplist ]for doc in docs]\n",
    "\n",
    "# remove words only one character\n",
    "docs = [[token for token in doc if len(token)>1]for doc in docs]\n",
    "\n",
    "# lemmatizaion\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token)for token in doc]for doc in docs]\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find bigrams here us find entities like New York, or terms like machine learning\n",
    "# and we get machine_learning in output, spaces are replaced by \n",
    "# of course n grams can be captured, use a name entity method is also recomm\n",
    "# once find bigrams, they will add them to the original data, because they want to keep them both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram(docs):\n",
    "    # Compute bigrams.\n",
    "    from gensim.models import Phrases\n",
    "\n",
    "    # Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "    bigram = Phrases(docs, min_count=20)\n",
    "    for idx in range(len(docs)):\n",
    "        for token in bigram[docs[idx]]:\n",
    "            if '_' in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                docs[idx].append(token)\n",
    "\n",
    "def another_way(docs):\n",
    "    from nltk import bigrams\n",
    "\n",
    "    for idx in range(len(docs)):\n",
    "        bigram = list(bigrams(docs[idx]))\n",
    "        bigram = [f\"{i[0]}_{i[1]}\" for i in bigram]\n",
    "        docs[idx].extend(bigram) # extend modify the list itself and returns None, so do not use it to assign a value\n",
    "\n",
    "another_way(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocab(docs)\n",
    "bow = build_bow_matrix(docs,vocab)\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 14162\n",
      "Number of documents: 1740\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shown in run_ida py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
